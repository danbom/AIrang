{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NarrativeKoGPT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danbom/AIrang/blob/eunyoung/NarrativeKoGPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0p6JShGUnpZ"
      },
      "source": [
        "# NarrativeKoGPT2 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV7_Ye-1UuS2"
      },
      "source": [
        "## 1.Google Drive 연동\n",
        "- 모델 파일과 학습 데이터가 저장 되어있는 구글 드라이브의 디렉토리와 Colab을 연동.  \n",
        "- 좌측상단 메뉴에서 런타임-> 런타임 유형 변경 -> 하드웨어 가속기 -> GPU 선택 후 저장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18LqQI0SVNX9"
      },
      "source": [
        "### 1.1 GPU 연동 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmKD5vgYUeTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e205e3dc-c1ed-446b-c933-6d978c97b75f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jan  7 00:41:27 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi2gIIroVXeS"
      },
      "source": [
        "### 1.2 Google Drive 연동\n",
        "아래 코드를 실행후 나오는 URL을 클릭하여 나오는 인증 코드 입력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2tPgkJzUmBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2997dc78-33ce-4814-e8fa-c13b6b2dfa26"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg07ZiFiVjJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff396a8c-851d-4a87-8a61-adf9f3eb61b8"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpCSwfFGkRx7"
      },
      "source": [
        "**Colab 디렉토리 아래 NarrativeKoGPT2 경로 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7arJ4k2XLG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faef2df0-515c-44c0-a111-e00c901a5bec"
      },
      "source": [
        "!ls drive/'My Drive'/'Colab Notebooks'/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NarrativeKoGPT2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVmhNd21kse2"
      },
      "source": [
        "**필요 패키지들 설치**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDrIL81uXPB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0500314-d8f0-4ec7-e5ce-46b0d0900beb"
      },
      "source": [
        "!pip install -r drive/'My Drive'/'Colab Notebooks'/NarrativeKoGPT2/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonnlp>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 4.3MB/s \n",
            "\u001b[?25hCollecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/bb/54cbabe428351c06d10903c658878d29ee7026efbe45133fd133598d6eb6/mxnet-1.7.0.post1-py2.py3-none-manylinux2014_x86_64.whl (55.0MB)\n",
            "\u001b[K     |████████████████████████████████| 55.0MB 57kB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 4)) (1.7.0+cu101)\n",
            "Collecting transformers==2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 6)) (4.41.1)\n",
            "Collecting kss\n",
            "  Downloading https://files.pythonhosted.org/packages/55/52/e3216b68094a73c39dfb037f4f2f5ba21177e8178f334829f985dbf12194/kss-2.2.0.2-py3-none-any.whl\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 44.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (1.19.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (0.29.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (20.8)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 4)) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 4)) (0.16.0)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 45.1MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/c6/b4d9547a493ac2837f296f4a004dff6e7136cf6750d181769b8a61d63813/boto3-1.16.50.tar.gz (100kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 8)) (3.12.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (3.0.4)\n",
            "Collecting botocore<1.20.0,>=1.19.50\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/19/113344007b2c1e4401f404f8e809f6151c00b164d78b164e4b688d30fc67/botocore-1.19.50-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 44.3MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 8)) (51.0.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.50->boto3->transformers==2.4.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (2.8.1)\n",
            "Building wheels for collected packages: gluonnlp, boto3, sacremoses\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588512 sha256=95875c549d4ecbb11395d89c2e78242efc748f485296fcf306f447dcf6120d0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.16.50-py2.py3-none-any.whl size=128710 sha256=5b91f47b40aa5ed6538e15d88cb7ded1ef43b71ea8bc33bcb2452c1d050bd9dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/7a/70/33e66d82b7829099a8f9a9abbb669195cf10e4a7b25ff2bc2a\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=9fb9a5d1f140bf7b2689602bccca1b4fa97ae1505c39f7545ab8d6f90834ca7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built gluonnlp boto3 sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.50 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gluonnlp, graphviz, mxnet, sentencepiece, tokenizers, jmespath, botocore, s3transfer, boto3, sacremoses, transformers, kss, tensorboardX\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed boto3-1.16.50 botocore-1.19.50 gluonnlp-0.10.0 graphviz-0.8.4 jmespath-0.10.0 kss-2.2.0.2 mxnet-1.7.0.post1 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 tensorboardX-2.1 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSCmVmaTlZ5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4d5324-407b-406c-b58f-3e3d570e37de"
      },
      "source": [
        "import os\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/')\n",
        "print(os.getcwd())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hnSOCChk9lU"
      },
      "source": [
        "## 2.KoGPT2 Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL6xVLtHn6vK"
      },
      "source": [
        "### 2.1.Import Package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IGI-Rcakhsw"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader # 데이터로더\n",
        "from gluonnlp.data import SentencepieceTokenizer \n",
        "from NarrativeKoGPT2.kogpt2.utils import get_tokenizer\n",
        "from NarrativeKoGPT2.kogpt2.utils import download, tokenizer\n",
        "from NarrativeKoGPT2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
        "from NarrativeKoGPT2.util.data import NovelDataset\n",
        "import gluonnlp\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01vGgfaNIDT_"
      },
      "source": [
        "**torch GPU 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztdqTt3OIBPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8ceeac-9512-42c3-99ac-ed3a300458c4"
      },
      "source": [
        "print(torch.cuda.device_count())  # GPU deviec count check"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G20dHg4mn5x4"
      },
      "source": [
        "### 2.2. koGPT-2 Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPoFzMKkk8eB"
      },
      "source": [
        "ctx= 'cuda'#'cuda' #'cpu' #학습 Device CPU or GPU. colab의 경우 GPU 사용\n",
        "cachedir='~/kogpt2/' # KoGPT-2 모델 다운로드 경로\n",
        "epoch =200  # 학습 epoch\n",
        "save_path = 'drive/My Drive/Colab Notebooks/NarrativeKoGPT2/checkpoint/'\n",
        "#use_cuda = True # Colab내 GPU 사용을 위한 값\n",
        "\n",
        "pytorch_kogpt2 = {\n",
        "    'url':\n",
        "    'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
        "    'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
        "    'chksum': '676e9bcfa7'\n",
        "}\n",
        "kogpt2_config = {\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"layer_norm_epsilon\": 1e-05,\n",
        "    \"n_ctx\": 1024,\n",
        "    \"n_embd\": 768,\n",
        "    \"n_head\": 12,\n",
        "    \"n_layer\": 12,\n",
        "    \"n_positions\": 1024,\n",
        "    \"vocab_size\": 50000\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xIXykCtn45d"
      },
      "source": [
        "### 2.3 Model and Vocab Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvtLJGh5o0MZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6fddc6d-64dc-4e95-e25d-1a0efe013dd0"
      },
      "source": [
        "# download model\n",
        "model_info = pytorch_kogpt2\n",
        "model_path = download(model_info['url'],\n",
        "                       model_info['fname'],\n",
        "                       model_info['chksum'],\n",
        "                       cachedir=cachedir)\n",
        "# download vocab\n",
        "vocab_info = tokenizer\n",
        "vocab_path = download(vocab_info['url'],\n",
        "                       vocab_info['fname'],\n",
        "                       vocab_info['chksum'],\n",
        "                       cachedir=cachedir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu7-2csBpLQR"
      },
      "source": [
        "### 2.4.KoGPT-2 Model Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5AK_S6spqwQ"
      },
      "source": [
        "# KoGPT-2 언어 모델 학습을 위한 GPT2LMHeadModel 선언\n",
        "kogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "# model_path로부터 다운로드 받은 내용을 load_state_dict으로 업로드\n",
        "kogpt2model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "device = torch.device(ctx)\n",
        "kogpt2model.to(device)\n",
        "\n",
        "# kogpt2model.eval()\n",
        "# 추가로 학습하기 위해 .train() 사용\n",
        "kogpt2model.train()\n",
        "vocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
        "                                                     mask_token=None,\n",
        "                                                     sep_token=None,\n",
        "                                                     cls_token=None,\n",
        "                                                     unknown_token=None,\n",
        "                                                     padding_token='<pad>',\n",
        "                                                     bos_token=None,\n",
        "                                                     eos_token=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rnV010Wq9Xw"
      },
      "source": [
        "### 2.5. Get Batch Data using DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukfj9FPHpwfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6404518b-8d82-4acf-ac25-c283a27dbfbc"
      },
      "source": [
        "tok_path = get_tokenizer()\n",
        "model, vocab = kogpt2model, vocab_b_obj\n",
        "sentencepieceTokenizer = SentencepieceTokenizer(tok_path)\n",
        "\n",
        "#os.chdir(\"../\")\n",
        "data_file_path = 'drive/My Drive/Colab Notebooks/NarrativeKoGPT2/data/bm_novel_1/prerpcessed_bm_novel_utf8_3.txt'\n",
        "batch_size = 2\n",
        "novel_dataset = NovelDataset(data_file_path, vocab, sentencepieceTokenizer)\n",
        "#novel_data_loader = DataLoader(novel_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "novel_data_loader = DataLoader(novel_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=lambda x: x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RFndCOIrLS0"
      },
      "source": [
        "### 2.6. Learning rate, Loss function, Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pY_o_C-qBhz"
      },
      "source": [
        "learning_rate = 1e-5\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMadZrwzXjbM"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "def get_gpu_memory_map():\n",
        "    \"\"\"Get the current gpu usage.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    usage: dict\n",
        "        Keys are device ids as integers.\n",
        "        Values are memory usage as integers in MB.\n",
        "    \"\"\"\n",
        "    result = subprocess.check_output(\n",
        "        [\n",
        "            'nvidia-smi', '--query-gpu=memory.used',\n",
        "            '--format=csv,nounits,noheader'\n",
        "        ], encoding='utf-8')\n",
        "    # Convert lines into a dictionary\n",
        "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
        "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
        "    return gpu_memory_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgd28DRhthzo"
      },
      "source": [
        "### 2.7. KoGPT-2 Transfer Laerning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYkDU-cbrduY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7beb46a3-8029-4558-ea55-2d72522889c7"
      },
      "source": [
        "# from tensorboardX import SummaryWriter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from NarrativeKoGPT2.sampling import sample_sequence\n",
        "summary = SummaryWriter()\n",
        "avg_loss = (0.0, 0.0)\n",
        "\n",
        "print('KoGPT-2 Transfer Learning Start')\n",
        "epoch=50\n",
        "\n",
        "for epoch in range(epoch):\n",
        "  count = 0\n",
        "  for data in novel_data_loader:\n",
        "    # data = datas[0] ### lyric version 2에 있는 코드\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (type(data) != torch.Tensor):\n",
        "      while (len(data[0]) != len(data[1])):\n",
        "        if (len(data[0]) > len(data[1])):\n",
        "          data[1].append(1)\n",
        "        else :\n",
        "          data[0].append(1)\n",
        "      # data = torch.tensor(data)\n",
        "      data[0] = torch.tensor(data[0])\n",
        "      data[1] = torch.tensor(data[1])\n",
        "\n",
        "    data = torch.stack(data) # list of Tensor로 구성되어 있기 때문에 list를 stack을 통해 변환해준다.\n",
        "\n",
        "    data= data.transpose(1,0)\n",
        "    data= data.to(ctx)\n",
        "    model = model.to(ctx)\n",
        "    \n",
        "    outputs = model(data, labels=data)\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    avg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
        "    loss *= datas[2][0] # 특별 socre 부분\n",
        "    \n",
        "    loss = loss.to(ctx)\n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "\n",
        "    if count %100 ==0:\n",
        "        print('epoch no.{} train no.{}  loss = {}   avg_loss = {}' . format(epoch, count+1, loss, avg_loss[0] / avg_loss[1]))\n",
        "        # torch.save(model,save_path+'checkpoint_{}_{}.tar'.format(epoch,count))\n",
        "        summary.add_scalar('loss/avg_loss', avg_loss[0] / avg_loss[1], count)\n",
        "        summary.add_scalar('loss/loss', loss, count)\n",
        "   \n",
        "    # generator 진행\n",
        "    if (count > 0 and count % 500 == 0) or (len(data) < batch_size):\n",
        "        sent = sample_sequence(model.to(\"cpu\"), sentencepieceTokenizer, vocab, sent=\"우리\", text_size=100, temperature=0.7, top_p=0.9, top_k=100)\n",
        "        sent = sent.replace(\"<unused0>\", \"\\n\")\n",
        "        sent = sent.replace(\"//\", \"\\n\")\n",
        "        print(sent)\n",
        "\n",
        "        summary.add_text('Text', sent, count)\n",
        "\n",
        "        if count > 500000:\n",
        "            now = [int(n) for n in os.listdir(samples)]\n",
        "            now = max(now)\n",
        "            f = open(samples + str(now + 1), 'w', encoding=\"utf-8\")\n",
        "            f.write(sent)\n",
        "            f.close()\n",
        "      # 추론 및 학습 재개를 위한 일반 체크포인트 저장하기\n",
        "    if (count >0 and count%100==0) or (len(data) < batch_size):\n",
        "      torch.save({\n",
        "        'epoch': epoch,\n",
        "        'train_no': count,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss':loss\n",
        "      }, save_path+'narrativeKoGPT2_checkpoint.tar')\n",
        "\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KoGPT-2 Transfer Learning Start\n",
            "epoch no.0 train no.1  loss = 6.2840776443481445\n",
            "epoch no.0 train no.101  loss = 7.443725109100342\n",
            "epoch no.0 train no.201  loss = 3.8746390342712402\n",
            "epoch no.0 train no.301  loss = 4.118159770965576\n",
            "epoch no.0 train no.401  loss = 3.7979559898376465\n",
            "epoch no.0 train no.501  loss = 4.72245454788208\n",
            "to_tokens: ['</s>', '<unk>', '</s>', '▁']\n",
            "우리</s>\n",
            "\n",
            "epoch no.0 train no.601  loss = 3.611882209777832\n",
            "epoch no.0 train no.701  loss = 4.238971710205078\n",
            "epoch no.0 train no.801  loss = 3.700378894805908\n",
            "epoch no.1 train no.1  loss = 4.582493305206299\n",
            "epoch no.1 train no.101  loss = 4.4023051261901855\n",
            "epoch no.1 train no.201  loss = 3.47839617729187\n",
            "epoch no.1 train no.301  loss = 5.305163860321045\n",
            "epoch no.1 train no.401  loss = 4.164834499359131\n",
            "epoch no.1 train no.501  loss = 4.615095138549805\n",
            "to_tokens: ['<s>', '<unk>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.1 train no.601  loss = 4.739813327789307\n",
            "epoch no.1 train no.701  loss = 4.9747090339660645\n",
            "epoch no.1 train no.801  loss = 4.11260461807251\n",
            "epoch no.2 train no.1  loss = 4.797048091888428\n",
            "epoch no.2 train no.101  loss = 5.022586822509766\n",
            "epoch no.2 train no.201  loss = 4.066056728363037\n",
            "epoch no.2 train no.301  loss = 2.766556739807129\n",
            "epoch no.2 train no.401  loss = 3.72792649269104\n",
            "epoch no.2 train no.501  loss = 5.2621169090271\n",
            "to_tokens: ['<s>', '</s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.2 train no.601  loss = 3.1583292484283447\n",
            "epoch no.2 train no.701  loss = 4.076456546783447\n",
            "epoch no.2 train no.801  loss = 4.524945259094238\n",
            "epoch no.3 train no.1  loss = 4.424160003662109\n",
            "epoch no.3 train no.101  loss = 4.373834609985352\n",
            "epoch no.3 train no.201  loss = 4.303463459014893\n",
            "epoch no.3 train no.301  loss = 4.480708599090576\n",
            "epoch no.3 train no.401  loss = 3.2666451930999756\n",
            "epoch no.3 train no.501  loss = 4.0541300773620605\n",
            "to_tokens: ['<s>', '<s>', '▁', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.3 train no.601  loss = 5.396495819091797\n",
            "epoch no.3 train no.701  loss = 3.127995491027832\n",
            "epoch no.3 train no.801  loss = 4.237916469573975\n",
            "epoch no.4 train no.1  loss = 4.916910171508789\n",
            "epoch no.4 train no.101  loss = 4.531731605529785\n",
            "epoch no.4 train no.201  loss = 3.8461692333221436\n",
            "epoch no.4 train no.301  loss = 3.235696315765381\n",
            "epoch no.4 train no.401  loss = 3.5947349071502686\n",
            "epoch no.4 train no.501  loss = 3.6432652473449707\n",
            "to_tokens: ['<s>', '</s>', '</s>', '</s>']\n",
            "우리 </s>\n",
            "\n",
            "epoch no.4 train no.601  loss = 4.166432857513428\n",
            "epoch no.4 train no.701  loss = 5.1814141273498535\n",
            "epoch no.4 train no.801  loss = 4.382522106170654\n",
            "epoch no.5 train no.1  loss = 2.8289437294006348\n",
            "epoch no.5 train no.101  loss = 4.760750770568848\n",
            "epoch no.5 train no.201  loss = 3.960493803024292\n",
            "epoch no.5 train no.301  loss = 5.207424640655518\n",
            "epoch no.5 train no.401  loss = 4.563470363616943\n",
            "epoch no.5 train no.501  loss = 4.590129852294922\n",
            "to_tokens: ['<s>', '</s>', '</s>', '</s>']\n",
            "우리 </s>\n",
            "\n",
            "epoch no.5 train no.601  loss = 3.3421518802642822\n",
            "epoch no.5 train no.701  loss = 2.885298490524292\n",
            "epoch no.5 train no.801  loss = 4.482700824737549\n",
            "epoch no.6 train no.1  loss = 4.768911838531494\n",
            "epoch no.6 train no.101  loss = 3.1300652027130127\n",
            "epoch no.6 train no.201  loss = 2.570481300354004\n",
            "epoch no.6 train no.301  loss = 2.7511653900146484\n",
            "epoch no.6 train no.401  loss = 2.4636523723602295\n",
            "epoch no.6 train no.501  loss = 4.147102355957031\n",
            "to_tokens: ['<s>', '</s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.6 train no.601  loss = 4.587951183319092\n",
            "epoch no.6 train no.701  loss = 4.859490871429443\n",
            "epoch no.6 train no.801  loss = 3.904594659805298\n",
            "epoch no.7 train no.1  loss = 4.799546718597412\n",
            "epoch no.7 train no.101  loss = 4.551135063171387\n",
            "epoch no.7 train no.201  loss = 4.766716003417969\n",
            "epoch no.7 train no.301  loss = 3.1151981353759766\n",
            "epoch no.7 train no.401  loss = 3.021547794342041\n",
            "epoch no.7 train no.501  loss = 2.9794366359710693\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리 </s>\n",
            "\n",
            "epoch no.7 train no.601  loss = 4.590475559234619\n",
            "epoch no.7 train no.701  loss = 4.538191795349121\n",
            "epoch no.7 train no.801  loss = 4.355747699737549\n",
            "epoch no.8 train no.1  loss = 5.026180267333984\n",
            "epoch no.8 train no.101  loss = 4.263510227203369\n",
            "epoch no.8 train no.201  loss = 4.845674991607666\n",
            "epoch no.8 train no.301  loss = 4.864033222198486\n",
            "epoch no.8 train no.401  loss = 4.9246063232421875\n",
            "epoch no.8 train no.501  loss = 4.158803939819336\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리 </s>\n",
            "\n",
            "epoch no.8 train no.601  loss = 4.093027114868164\n",
            "epoch no.8 train no.701  loss = 4.206722736358643\n",
            "epoch no.8 train no.801  loss = 3.9965195655822754\n",
            "epoch no.9 train no.1  loss = 4.377760410308838\n",
            "epoch no.9 train no.101  loss = 3.997492551803589\n",
            "epoch no.9 train no.201  loss = 4.881567001342773\n",
            "epoch no.9 train no.301  loss = 4.658753395080566\n",
            "epoch no.9 train no.401  loss = 4.668550968170166\n",
            "epoch no.9 train no.501  loss = 3.656386375427246\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리 </s>\n",
            "\n",
            "epoch no.9 train no.601  loss = 4.5617499351501465\n",
            "epoch no.9 train no.701  loss = 4.8736371994018555\n",
            "epoch no.9 train no.801  loss = 4.856318473815918\n",
            "epoch no.10 train no.1  loss = 4.312017917633057\n",
            "epoch no.10 train no.101  loss = 3.9051151275634766\n",
            "epoch no.10 train no.201  loss = 3.125755786895752\n",
            "epoch no.10 train no.301  loss = 3.2301185131073\n",
            "epoch no.10 train no.401  loss = 2.8885881900787354\n",
            "epoch no.10 train no.501  loss = 5.235879898071289\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.10 train no.601  loss = 4.187460422515869\n",
            "epoch no.10 train no.701  loss = 5.013688087463379\n",
            "epoch no.10 train no.801  loss = 3.5917184352874756\n",
            "epoch no.11 train no.1  loss = 5.111368656158447\n",
            "epoch no.11 train no.101  loss = 4.093039512634277\n",
            "epoch no.11 train no.201  loss = 4.564176559448242\n",
            "epoch no.11 train no.301  loss = 4.479529857635498\n",
            "epoch no.11 train no.401  loss = 4.0492377281188965\n",
            "epoch no.11 train no.501  loss = 4.5502495765686035\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리 </s>\n",
            "\n",
            "epoch no.11 train no.601  loss = 3.6150405406951904\n",
            "epoch no.11 train no.701  loss = 2.8345189094543457\n",
            "epoch no.11 train no.801  loss = 4.599291801452637\n",
            "epoch no.12 train no.1  loss = 4.401015281677246\n",
            "epoch no.12 train no.101  loss = 4.812049865722656\n",
            "epoch no.12 train no.201  loss = 2.903705358505249\n",
            "epoch no.12 train no.301  loss = 4.8774309158325195\n",
            "epoch no.12 train no.401  loss = 4.570140361785889\n",
            "epoch no.12 train no.501  loss = 2.8376424312591553\n",
            "to_tokens: ['<s>', '▁', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.12 train no.601  loss = 3.812465190887451\n",
            "epoch no.12 train no.701  loss = 2.682722806930542\n",
            "epoch no.12 train no.801  loss = 4.590162754058838\n",
            "epoch no.13 train no.1  loss = 4.127405643463135\n",
            "epoch no.13 train no.101  loss = 2.6508045196533203\n",
            "epoch no.13 train no.201  loss = 4.349786758422852\n",
            "epoch no.13 train no.301  loss = 4.353955268859863\n",
            "epoch no.13 train no.401  loss = 4.123908519744873\n",
            "epoch no.13 train no.501  loss = 4.592819690704346\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.13 train no.601  loss = 2.632553815841675\n",
            "epoch no.13 train no.701  loss = 3.4690544605255127\n",
            "epoch no.13 train no.801  loss = 3.0823471546173096\n",
            "epoch no.14 train no.1  loss = 4.233460426330566\n",
            "epoch no.14 train no.101  loss = 3.4080328941345215\n",
            "epoch no.14 train no.201  loss = 3.6218507289886475\n",
            "epoch no.14 train no.301  loss = 4.635883808135986\n",
            "epoch no.14 train no.401  loss = 4.660633563995361\n",
            "epoch no.14 train no.501  loss = 4.867965221405029\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리  </s>\n",
            "\n",
            "epoch no.14 train no.601  loss = 4.764460563659668\n",
            "epoch no.14 train no.701  loss = 3.212402105331421\n",
            "epoch no.14 train no.801  loss = 3.264988899230957\n",
            "epoch no.15 train no.1  loss = 5.149043560028076\n",
            "epoch no.15 train no.101  loss = 3.5523712635040283\n",
            "epoch no.15 train no.201  loss = 3.1712465286254883\n",
            "epoch no.15 train no.301  loss = 4.703962802886963\n",
            "epoch no.15 train no.401  loss = 3.1987640857696533\n",
            "epoch no.15 train no.501  loss = 3.7969071865081787\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.15 train no.601  loss = 4.582452297210693\n",
            "epoch no.15 train no.701  loss = 3.676556348800659\n",
            "epoch no.15 train no.801  loss = 3.3383970260620117\n",
            "epoch no.16 train no.1  loss = 4.775218486785889\n",
            "epoch no.16 train no.101  loss = 4.318816661834717\n",
            "epoch no.16 train no.201  loss = 4.730848789215088\n",
            "epoch no.16 train no.301  loss = 3.7060487270355225\n",
            "epoch no.16 train no.401  loss = 4.185064315795898\n",
            "epoch no.16 train no.501  loss = 3.1152565479278564\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.16 train no.601  loss = 2.669111490249634\n",
            "epoch no.16 train no.701  loss = 4.711983680725098\n",
            "epoch no.16 train no.801  loss = 2.614978313446045\n",
            "epoch no.17 train no.1  loss = 4.476382255554199\n",
            "epoch no.17 train no.101  loss = 2.7103447914123535\n",
            "epoch no.17 train no.201  loss = 4.845195770263672\n",
            "epoch no.17 train no.301  loss = 3.5503010749816895\n",
            "epoch no.17 train no.401  loss = 4.715511798858643\n",
            "epoch no.17 train no.501  loss = 4.432117938995361\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.17 train no.601  loss = 4.593489646911621\n",
            "epoch no.17 train no.701  loss = 4.304963111877441\n",
            "epoch no.17 train no.801  loss = 2.9984166622161865\n",
            "epoch no.18 train no.1  loss = 4.71575927734375\n",
            "epoch no.18 train no.101  loss = 4.217609405517578\n",
            "epoch no.18 train no.201  loss = 5.057132244110107\n",
            "epoch no.18 train no.301  loss = 4.6271281242370605\n",
            "epoch no.18 train no.401  loss = 3.523230791091919\n",
            "epoch no.18 train no.501  loss = 4.373581886291504\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리  </s>\n",
            "\n",
            "epoch no.18 train no.601  loss = 4.862149715423584\n",
            "epoch no.18 train no.701  loss = 3.754997730255127\n",
            "epoch no.18 train no.801  loss = 4.857371807098389\n",
            "epoch no.19 train no.1  loss = 5.21857213973999\n",
            "epoch no.19 train no.101  loss = 3.443250894546509\n",
            "epoch no.19 train no.201  loss = 3.5624406337738037\n",
            "epoch no.19 train no.301  loss = 4.363790035247803\n",
            "epoch no.19 train no.401  loss = 2.949068307876587\n",
            "epoch no.19 train no.501  loss = 3.292588949203491\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.19 train no.601  loss = 4.3389716148376465\n",
            "epoch no.19 train no.701  loss = 4.278998851776123\n",
            "epoch no.19 train no.801  loss = 4.232077598571777\n",
            "epoch no.20 train no.1  loss = 5.042304039001465\n",
            "epoch no.20 train no.101  loss = 3.772336721420288\n",
            "epoch no.20 train no.201  loss = 4.946346759796143\n",
            "epoch no.20 train no.301  loss = 4.4860005378723145\n",
            "epoch no.20 train no.401  loss = 3.7407209873199463\n",
            "epoch no.20 train no.501  loss = 3.6927433013916016\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.20 train no.601  loss = 4.816032886505127\n",
            "epoch no.20 train no.701  loss = 3.7523040771484375\n",
            "epoch no.20 train no.801  loss = 4.769159317016602\n",
            "epoch no.21 train no.1  loss = 2.4971351623535156\n",
            "epoch no.21 train no.101  loss = 2.624539852142334\n",
            "epoch no.21 train no.201  loss = 4.730221748352051\n",
            "epoch no.21 train no.301  loss = 4.8687520027160645\n",
            "epoch no.21 train no.401  loss = 4.148611068725586\n",
            "epoch no.21 train no.501  loss = 3.958045482635498\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.21 train no.601  loss = 4.626883029937744\n",
            "epoch no.21 train no.701  loss = 3.4786410331726074\n",
            "epoch no.21 train no.801  loss = 2.940751552581787\n",
            "epoch no.22 train no.1  loss = 3.1332955360412598\n",
            "epoch no.22 train no.101  loss = 4.936009407043457\n",
            "epoch no.22 train no.201  loss = 4.824213027954102\n",
            "epoch no.22 train no.301  loss = 4.5208234786987305\n",
            "epoch no.22 train no.401  loss = 4.82483434677124\n",
            "epoch no.22 train no.501  loss = 3.9350850582122803\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.22 train no.601  loss = 3.654629945755005\n",
            "epoch no.22 train no.701  loss = 2.2811930179595947\n",
            "epoch no.22 train no.801  loss = 4.346675395965576\n",
            "epoch no.23 train no.1  loss = 4.032088756561279\n",
            "epoch no.23 train no.101  loss = 3.9351272583007812\n",
            "epoch no.23 train no.201  loss = 4.648437023162842\n",
            "epoch no.23 train no.301  loss = 4.281141757965088\n",
            "epoch no.23 train no.401  loss = 3.139439582824707\n",
            "epoch no.23 train no.501  loss = 4.521143913269043\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.23 train no.601  loss = 4.293888568878174\n",
            "epoch no.23 train no.701  loss = 4.765804290771484\n",
            "epoch no.23 train no.801  loss = 4.358051776885986\n",
            "epoch no.24 train no.1  loss = 4.092330455780029\n",
            "epoch no.24 train no.101  loss = 4.755876064300537\n",
            "epoch no.24 train no.201  loss = 4.225969314575195\n",
            "epoch no.24 train no.301  loss = 4.664159297943115\n",
            "epoch no.24 train no.401  loss = 3.7270467281341553\n",
            "epoch no.24 train no.501  loss = 2.60070538520813\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>', '</s>']\n",
            "우리리 </s>\n",
            "\n",
            "epoch no.24 train no.601  loss = 4.246815204620361\n",
            "epoch no.24 train no.701  loss = 4.654364109039307\n",
            "epoch no.24 train no.801  loss = 4.75573205947876\n",
            "epoch no.25 train no.1  loss = 3.9973948001861572\n",
            "epoch no.25 train no.101  loss = 2.8581454753875732\n",
            "epoch no.25 train no.201  loss = 4.9347381591796875\n",
            "epoch no.25 train no.301  loss = 4.716821193695068\n",
            "epoch no.25 train no.401  loss = 4.443959712982178\n",
            "epoch no.25 train no.501  loss = 4.015210151672363\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.25 train no.601  loss = 3.4458930492401123\n",
            "epoch no.25 train no.701  loss = 4.347675800323486\n",
            "epoch no.25 train no.801  loss = 4.738851547241211\n",
            "epoch no.26 train no.1  loss = 3.176358938217163\n",
            "epoch no.26 train no.101  loss = 2.4374845027923584\n",
            "epoch no.26 train no.201  loss = 4.374892711639404\n",
            "epoch no.26 train no.301  loss = 2.7486062049865723\n",
            "epoch no.26 train no.401  loss = 4.488500595092773\n",
            "epoch no.26 train no.501  loss = 5.3656134605407715\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.26 train no.601  loss = 4.485541820526123\n",
            "epoch no.26 train no.701  loss = 4.434778213500977\n",
            "epoch no.26 train no.801  loss = 4.476579666137695\n",
            "epoch no.27 train no.1  loss = 4.808241367340088\n",
            "epoch no.27 train no.101  loss = 4.604884624481201\n",
            "epoch no.27 train no.201  loss = 4.101003170013428\n",
            "epoch no.27 train no.301  loss = 4.669384002685547\n",
            "epoch no.27 train no.401  loss = 4.239251136779785\n",
            "epoch no.27 train no.501  loss = 4.729773044586182\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.27 train no.601  loss = 4.705440044403076\n",
            "epoch no.27 train no.701  loss = 4.578746795654297\n",
            "epoch no.27 train no.801  loss = 5.035863876342773\n",
            "epoch no.28 train no.1  loss = 4.543398380279541\n",
            "epoch no.28 train no.101  loss = 4.579153060913086\n",
            "epoch no.28 train no.201  loss = 3.680340528488159\n",
            "epoch no.28 train no.301  loss = 2.9861583709716797\n",
            "epoch no.28 train no.401  loss = 3.5446221828460693\n",
            "epoch no.28 train no.501  loss = 4.780694484710693\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리 </s>\n",
            "\n",
            "epoch no.28 train no.601  loss = 4.212569713592529\n",
            "epoch no.28 train no.701  loss = 4.182456016540527\n",
            "epoch no.28 train no.801  loss = 3.845348596572876\n",
            "epoch no.29 train no.1  loss = 5.242755889892578\n",
            "epoch no.29 train no.101  loss = 3.23349666595459\n",
            "epoch no.29 train no.201  loss = 4.485604763031006\n",
            "epoch no.29 train no.301  loss = 3.672171115875244\n",
            "epoch no.29 train no.401  loss = 5.4684576988220215\n",
            "epoch no.29 train no.501  loss = 2.705549478530884\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.29 train no.601  loss = 4.3127760887146\n",
            "epoch no.29 train no.701  loss = 4.395607948303223\n",
            "epoch no.29 train no.801  loss = 4.605562686920166\n",
            "epoch no.30 train no.1  loss = 4.31917142868042\n",
            "epoch no.30 train no.101  loss = 4.692604064941406\n",
            "epoch no.30 train no.201  loss = 4.670819282531738\n",
            "epoch no.30 train no.301  loss = 3.122685670852661\n",
            "epoch no.30 train no.401  loss = 5.357487678527832\n",
            "epoch no.30 train no.501  loss = 3.3550186157226562\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>', '</s>', '</s>', '▁', '▁']\n",
            "우리 <s></s>\n",
            "\n",
            "epoch no.30 train no.601  loss = 4.695339679718018\n",
            "epoch no.30 train no.701  loss = 3.8896961212158203\n",
            "epoch no.30 train no.801  loss = 3.588968276977539\n",
            "epoch no.31 train no.1  loss = 2.958582878112793\n",
            "epoch no.31 train no.101  loss = 3.3192319869995117\n",
            "epoch no.31 train no.201  loss = 4.535187244415283\n",
            "epoch no.31 train no.301  loss = 4.7319231033325195\n",
            "epoch no.31 train no.401  loss = 4.456843852996826\n",
            "epoch no.31 train no.501  loss = 3.1490190029144287\n",
            "to_tokens: ['<s>', '▁', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.31 train no.601  loss = 5.047946453094482\n",
            "epoch no.31 train no.701  loss = 2.3100674152374268\n",
            "epoch no.31 train no.801  loss = 4.81097412109375\n",
            "epoch no.32 train no.1  loss = 2.7248315811157227\n",
            "epoch no.32 train no.101  loss = 3.181597948074341\n",
            "epoch no.32 train no.201  loss = 4.04418420791626\n",
            "epoch no.32 train no.301  loss = 3.240234136581421\n",
            "epoch no.32 train no.401  loss = 4.246866703033447\n",
            "epoch no.32 train no.501  loss = 4.3167877197265625\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.32 train no.601  loss = 4.704514980316162\n",
            "epoch no.32 train no.701  loss = 4.3233561515808105\n",
            "epoch no.32 train no.801  loss = 4.360536098480225\n",
            "epoch no.33 train no.1  loss = 5.030526161193848\n",
            "epoch no.33 train no.101  loss = 4.57211446762085\n",
            "epoch no.33 train no.201  loss = 4.403738021850586\n",
            "epoch no.33 train no.301  loss = 4.454721450805664\n",
            "epoch no.33 train no.401  loss = 4.16353178024292\n",
            "epoch no.33 train no.501  loss = 4.287743091583252\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.33 train no.601  loss = 4.448058128356934\n",
            "epoch no.33 train no.701  loss = 3.0593419075012207\n",
            "epoch no.33 train no.801  loss = 2.469243049621582\n",
            "epoch no.34 train no.1  loss = 2.812612533569336\n",
            "epoch no.34 train no.101  loss = 5.2374372482299805\n",
            "epoch no.34 train no.201  loss = 3.7565419673919678\n",
            "epoch no.34 train no.301  loss = 4.402947425842285\n",
            "epoch no.34 train no.401  loss = 4.868887901306152\n",
            "epoch no.34 train no.501  loss = 4.878696441650391\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.34 train no.601  loss = 4.833834171295166\n",
            "epoch no.34 train no.701  loss = 5.094122409820557\n",
            "epoch no.34 train no.801  loss = 4.21092414855957\n",
            "epoch no.35 train no.1  loss = 3.8967478275299072\n",
            "epoch no.35 train no.101  loss = 4.188990592956543\n",
            "epoch no.35 train no.201  loss = 4.388797760009766\n",
            "epoch no.35 train no.301  loss = 3.13913893699646\n",
            "epoch no.35 train no.401  loss = 4.206756114959717\n",
            "epoch no.35 train no.501  loss = 3.4254684448242188\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>', '</s>']\n",
            "우리리</s>\n",
            "\n",
            "epoch no.35 train no.601  loss = 3.2810351848602295\n",
            "epoch no.35 train no.701  loss = 2.685643434524536\n",
            "epoch no.35 train no.801  loss = 4.2304487228393555\n",
            "epoch no.36 train no.1  loss = 3.944885730743408\n",
            "epoch no.36 train no.101  loss = 4.599870681762695\n",
            "epoch no.36 train no.201  loss = 4.116055965423584\n",
            "epoch no.36 train no.301  loss = 4.904364109039307\n",
            "epoch no.36 train no.401  loss = 5.34432315826416\n",
            "epoch no.36 train no.501  loss = 3.1021082401275635\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>', '</s>', '</s>', '▁']\n",
            "우리<s></s>\n",
            "\n",
            "epoch no.36 train no.601  loss = 4.119027137756348\n",
            "epoch no.36 train no.701  loss = 4.131665229797363\n",
            "epoch no.36 train no.801  loss = 3.815805435180664\n",
            "epoch no.37 train no.1  loss = 3.7974472045898438\n",
            "epoch no.37 train no.101  loss = 4.715795516967773\n",
            "epoch no.37 train no.201  loss = 4.303954601287842\n",
            "epoch no.37 train no.301  loss = 4.309089660644531\n",
            "epoch no.37 train no.401  loss = 4.56974458694458\n",
            "epoch no.37 train no.501  loss = 2.7677950859069824\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.37 train no.601  loss = 3.2391128540039062\n",
            "epoch no.37 train no.701  loss = 4.807432174682617\n",
            "epoch no.37 train no.801  loss = 4.3462934494018555\n",
            "epoch no.38 train no.1  loss = 4.192495346069336\n",
            "epoch no.38 train no.101  loss = 2.7825706005096436\n",
            "epoch no.38 train no.201  loss = 3.771799325942993\n",
            "epoch no.38 train no.301  loss = 4.143960475921631\n",
            "epoch no.38 train no.401  loss = 4.8901567459106445\n",
            "epoch no.38 train no.501  loss = 4.300466060638428\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.38 train no.601  loss = 4.720251083374023\n",
            "epoch no.38 train no.701  loss = 5.123663902282715\n",
            "epoch no.38 train no.801  loss = 4.376102447509766\n",
            "epoch no.39 train no.1  loss = 3.633362054824829\n",
            "epoch no.39 train no.101  loss = 2.2454726696014404\n",
            "epoch no.39 train no.201  loss = 3.5086183547973633\n",
            "epoch no.39 train no.301  loss = 4.806882381439209\n",
            "epoch no.39 train no.401  loss = 3.0844168663024902\n",
            "epoch no.39 train no.501  loss = 2.8218612670898438\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.39 train no.601  loss = 4.357872009277344\n",
            "epoch no.39 train no.701  loss = 3.895963668823242\n",
            "epoch no.39 train no.801  loss = 5.0053582191467285\n",
            "epoch no.40 train no.1  loss = 4.315042972564697\n",
            "epoch no.40 train no.101  loss = 4.067595958709717\n",
            "epoch no.40 train no.201  loss = 4.8590593338012695\n",
            "epoch no.40 train no.301  loss = 4.040342330932617\n",
            "epoch no.40 train no.401  loss = 4.319821357727051\n",
            "epoch no.40 train no.501  loss = 3.938657760620117\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.40 train no.601  loss = 3.8291687965393066\n",
            "epoch no.40 train no.701  loss = 4.116787433624268\n",
            "epoch no.40 train no.801  loss = 5.188889980316162\n",
            "epoch no.41 train no.1  loss = 3.239445924758911\n",
            "epoch no.41 train no.101  loss = 4.479025363922119\n",
            "epoch no.41 train no.201  loss = 4.32118558883667\n",
            "epoch no.41 train no.301  loss = 5.073482036590576\n",
            "epoch no.41 train no.401  loss = 4.447134017944336\n",
            "epoch no.41 train no.501  loss = 2.9690027236938477\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>']\n",
            "우리 </s>\n",
            "\n",
            "epoch no.41 train no.601  loss = 4.5202317237854\n",
            "epoch no.41 train no.701  loss = 3.648509979248047\n",
            "epoch no.41 train no.801  loss = 3.559349536895752\n",
            "epoch no.42 train no.1  loss = 4.587564468383789\n",
            "epoch no.42 train no.101  loss = 4.412358283996582\n",
            "epoch no.42 train no.201  loss = 4.178030014038086\n",
            "epoch no.42 train no.301  loss = 4.081300735473633\n",
            "epoch no.42 train no.401  loss = 3.995137929916382\n",
            "epoch no.42 train no.501  loss = 3.5512890815734863\n",
            "to_tokens: ['<s>', '<s>', '<s>', '<s>', '<s>', '</s>', '▁', '▁']\n",
            "우리 <s>  </s>\n",
            "\n",
            "epoch no.42 train no.601  loss = 2.8009097576141357\n",
            "epoch no.42 train no.701  loss = 3.534497022628784\n",
            "epoch no.42 train no.801  loss = 3.633119583129883\n",
            "epoch no.43 train no.1  loss = 4.8411993980407715\n",
            "epoch no.43 train no.101  loss = 4.931795597076416\n",
            "epoch no.43 train no.201  loss = 4.188615798950195\n",
            "epoch no.43 train no.301  loss = 4.657229900360107\n",
            "epoch no.43 train no.401  loss = 3.4974756240844727\n",
            "epoch no.43 train no.501  loss = 3.2121493816375732\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.43 train no.601  loss = 4.343853950500488\n",
            "epoch no.43 train no.701  loss = 4.068822383880615\n",
            "epoch no.43 train no.801  loss = 4.105629920959473\n",
            "epoch no.44 train no.1  loss = 3.9979169368743896\n",
            "epoch no.44 train no.101  loss = 4.412485599517822\n",
            "epoch no.44 train no.201  loss = 3.607921838760376\n",
            "epoch no.44 train no.301  loss = 3.8585190773010254\n",
            "epoch no.44 train no.401  loss = 4.925694942474365\n",
            "epoch no.44 train no.501  loss = 3.6017580032348633\n",
            "to_tokens: ['<s>', '<s>', '<s>', '<s>', '</s>', '</s>', '</s>']\n",
            "우리<s></s>\n",
            "\n",
            "epoch no.44 train no.601  loss = 3.838627576828003\n",
            "epoch no.44 train no.701  loss = 4.36238431930542\n",
            "epoch no.44 train no.801  loss = 4.376221656799316\n",
            "epoch no.45 train no.1  loss = 3.5769717693328857\n",
            "epoch no.45 train no.101  loss = 4.765558242797852\n",
            "epoch no.45 train no.201  loss = 4.244711875915527\n",
            "epoch no.45 train no.301  loss = 4.122805118560791\n",
            "epoch no.45 train no.401  loss = 3.4859514236450195\n",
            "epoch no.45 train no.501  loss = 4.057909965515137\n",
            "to_tokens: ['<s>', '<s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "우리<s></s>\n",
            "\n",
            "epoch no.45 train no.601  loss = 4.663743019104004\n",
            "epoch no.45 train no.701  loss = 4.658114910125732\n",
            "epoch no.45 train no.801  loss = 5.005537509918213\n",
            "epoch no.46 train no.1  loss = 3.7393481731414795\n",
            "epoch no.46 train no.101  loss = 4.326594352722168\n",
            "epoch no.46 train no.201  loss = 4.702392101287842\n",
            "epoch no.46 train no.301  loss = 4.150636196136475\n",
            "epoch no.46 train no.401  loss = 4.230553150177002\n",
            "epoch no.46 train no.501  loss = 4.312702655792236\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>']\n",
            "우리 </s>\n",
            "\n",
            "epoch no.46 train no.601  loss = 4.6214752197265625\n",
            "epoch no.46 train no.701  loss = 4.405219554901123\n",
            "epoch no.46 train no.801  loss = 3.583329439163208\n",
            "epoch no.47 train no.1  loss = 5.163747787475586\n",
            "epoch no.47 train no.101  loss = 4.660611629486084\n",
            "epoch no.47 train no.201  loss = 3.3535072803497314\n",
            "epoch no.47 train no.301  loss = 5.302347660064697\n",
            "epoch no.47 train no.401  loss = 4.706435680389404\n",
            "epoch no.47 train no.501  loss = 4.212067127227783\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.47 train no.601  loss = 3.440065622329712\n",
            "epoch no.47 train no.701  loss = 3.4250926971435547\n",
            "epoch no.47 train no.801  loss = 4.386033058166504\n",
            "epoch no.48 train no.1  loss = 5.027657985687256\n",
            "epoch no.48 train no.101  loss = 3.2039122581481934\n",
            "epoch no.48 train no.201  loss = 4.908458232879639\n",
            "epoch no.48 train no.301  loss = 4.510059833526611\n",
            "epoch no.48 train no.401  loss = 5.231358051300049\n",
            "epoch no.48 train no.501  loss = 3.2359068393707275\n",
            "to_tokens: ['<s>', '<s>', '<s>', '</s>']\n",
            "우리</s>\n",
            "\n",
            "epoch no.48 train no.601  loss = 3.9306089878082275\n",
            "epoch no.48 train no.701  loss = 3.1245548725128174\n",
            "epoch no.48 train no.801  loss = 4.324892997741699\n",
            "epoch no.49 train no.1  loss = 4.558137893676758\n",
            "epoch no.49 train no.101  loss = 3.767650604248047\n",
            "epoch no.49 train no.201  loss = 4.473267078399658\n",
            "epoch no.49 train no.301  loss = 4.393739223480225\n",
            "epoch no.49 train no.401  loss = 4.92056131362915\n",
            "epoch no.49 train no.501  loss = 2.9776577949523926\n",
            "to_tokens: ['<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '</s>']\n",
            "우리<s></s>\n",
            "\n",
            "epoch no.49 train no.601  loss = 3.11594820022583\n",
            "epoch no.49 train no.701  loss = 3.9923648834228516\n",
            "epoch no.49 train no.801  loss = 4.145017147064209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nCvuRs5prwK"
      },
      "source": [
        "summary.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz5Yr0DMqEI0"
      },
      "source": [
        "summary.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWdI4idxqGk_",
        "outputId": "81da99a9-5d53-4a88-eaad-a19566c712d4"
      },
      "source": [
        "!pip install tensorboard --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboard in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (51.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.2.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEZObL2fq_5j",
        "outputId": "8c5b1e3a-56ef-4c23-d616-7671f6525d78"
      },
      "source": [
        "!tensorboard dev upload --logdir runs \\\r\n",
        "--name \"My latest experiment\" \\\r\n",
        "--description \"Simple comparison of several hyperparameters\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-06 07:49:40.219446: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
            "the following directory:\n",
            "\n",
            "runs\n",
            "\n",
            "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
            "data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) yes\n",
            "\n",
            "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=373649185512-8v619h5kft38l4456nm2dj4ubeqsrvh6.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&state=I3tARIDNpPMrx7V0GpKxgNjyPED2kb&prompt=consent&access_type=offline\n",
            "Enter the authorization code: 4/1AY0e-g7SjAIh04PMg-fpnatoc2dQrKXzxEpID4cyYzLE9xcANlDbUEv6xuQ\n",
            "\n",
            "Upload started and will continue reading any new data as it's added to the logdir.\n",
            "\n",
            "To stop uploading, press Ctrl-C.\n",
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/hWePJcDORwKjWu5h2xOKgA/\n",
            "\n",
            "\u001b[1m[2021-01-06T07:50:02]\u001b[0m Started scanning logdir.\n",
            "\u001b[1m[2021-01-06T07:50:02]\u001b[0m Total uploaded: 172 scalars, 0 tensors, 0 binary objects\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
            "    sys.exit(run_main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/main.py\", line 75, in run_main\n",
            "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 289, in main\n",
            "    return runner(self.flags) or 0\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_subcommand.py\", line 671, in run\n",
            "    return _run(flags, self._experiment_url_callback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_subcommand.py\", line 140, in _run\n",
            "    intent.execute(server_info, channel)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_subcommand.py\", line 485, in execute\n",
            "    if not self.dry_run and uploader.has_data():\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader.py\", line 166, in has_data\n",
            "    return self._tracker.has_data()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/upload_tracker.py\", line 307, in has_data\n",
            "    return self._stats.has_data()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
